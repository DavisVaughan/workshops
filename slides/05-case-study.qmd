---
title: "5 - Case Study"
subtitle: "Machine learning with tidymodels"
format:
  revealjs: 
    slide-number: true
    footer: <https://workshops.tidymodels.org>
    include-before-body: header.html
    theme: [default, tidymodels.scss]
knitr:
  opts_chunk: 
    echo: true
---

```{r}
#| include: false
#| file: setup.R
```

## Loading the packages

```{r hello-tidymodels}
library(tidymodels)
library(ongoal)

tidymodels_prefer()

on_goal <- 
  on_goal %>% 
  filter(season == "20152016") %>% 
  select(-season, -date_time, -distance, -angle, -behind_goal_line)
```

## Splitting the NHL data `r I(hexes(c("rsample", "dplyr")))`

```{r split}
nhl_split <- initial_split(on_goal, prop = 3/4)
nhl_split

nhl_train <- training(nhl_split)
nhl_test  <- testing(nhl_split)

c(training = nrow(nhl_train), testing = nrow(nhl_test))
```

# Validation split

```{r val}
set.seed(234)
nhl_split <- validation_split(nhl_train, prop = 0.80)
nhl_split
```

# A first model

```{r logistic-fit}
spec_glm <- logistic_reg() %>% set_engine("glm")
spec_glm
```

# A first recipe `r I(hexes(c("recipes")))`

```{r base-recipe}
nhl_rec <- 
  recipe(on_goal ~ ., data = nhl_train)

# If ncol(data) is large, you can use
# recipe(data = nhl_train)
```

Based on the formula, the function assigns columns to roles of "outcome" or "predictor"

# A first recipe `r I(hexes(c("recipes")))`

```{r rec-summary}
summary(nhl_rec)
```


# Create indicator variables `r I(hexes(c("recipes")))`

```{r}
nhl_rec <- 
  recipe(on_goal ~ ., data = nhl_train) %>% 
  step_dummy(all_nominal_predictors()) #<<
```

For any factor or character predictors, make binary indicators.

There are *many* recipe steps that can convert categorical predictors to numeric columns.

# Filter out constant columns `r I(hexes(c("recipes")))`

```{r}
nhl_rec <- 
  recipe(on_goal ~ ., data = nhl_train) %>% 
  step_dummy(all_nominal_predictors()) %>% 
  step_zv(all_predictors()) #<<
```

In case there is factor level that never was observed in the trainnig data, we can delete any *zero-variance* predictors that have a single unique value.

<!--Note that the selector chooses all columns with a role of "predictor".-->

# Normalization `r I(hexes(c("recipes")))`

```{r rec-norm, eval = FALSE}
nhl_rec <- 
  recipe(on_goal ~ ., data = nhl_train) %>% 
  step_dummy(all_nominal_predictors()) %>% 
  step_zv(all_predictors()) %>% 
  step_normalize(all_numeric_predictors()) #<<
```

This centers and scales the numeric predictors.

Note that this will use the training set to estimate the means and standard deviations of the data.

All data put through the recipe will be normalized using those statistics (there is no re-estimation).

# Reduce correlation `r I(hexes(c("recipes")))`

```{r, eval = FALSE}
nhl_rec <- 
  recipe(on_goal ~ ., data = nhl_train) %>% 
  step_dummy(all_nominal_predictors()) %>% 
  step_zv(all_predictors()) %>% 
  step_normalize(all_numeric_predictors()) %>% 
  step_corr(all_numeric_predictors(), threshold = 0.9) #<<
```

To deal with highly correlated predictors, find the minimum predictor set to remove to make the pairwise correlations less than 0.9.

# Other possible steps `r I(hexes(c("recipes")))`

```{r, eval = FALSE}
nhl_rec <- 
  recipe(on_goal ~ ., data = nhl_train) %>% 
  step_dummy(all_nominal_predictors()) %>% 
  step_zv(all_predictors()) %>% 
  step_normalize(all_numeric_predictors()) %>% 
  step_pca(all_numeric_predictors()) #<<
```

PCA feature extraction...

# Other possible steps `r I(hexes(c("recipes", "embed")))`

```{r, eval = FALSE}
nhl_rec <- 
  recipe(on_goal ~ ., data = nhl_train) %>% 
  step_dummy(all_nominal_predictors()) %>% 
  step_zv(all_predictors()) %>% 
  step_normalize(all_numeric_predictors()) %>% 
  step_umap(all_numeric_predictors(), outcome = on_goal) #<<
```

A fancy machine learning supervised dimension reduction technique

# Other possible steps `r I(hexes(c("recipes")))`

```{r, eval = FALSE}
nhl_rec <- 
  recipe(on_goal ~ ., data = nhl_train) %>% 
  step_dummy(all_nominal_predictors()) %>% 
  step_zv(all_predictors()) %>% 
  step_normalize(all_numeric_predictors()) %>% 
  step_ns(angle, deg_free = 10) #<<
```

Nonlinear transforms like *natural splines* and so on.

# What do we do with the player data?

There are `r length(unique(nhl_train$player))` unique player values in our training set.

-   We *could* make the full set of indicator variables...
-   Or using [feature hashing](https://www.tmwr.org/categorical.html#feature-hashing) to make a subset.

Instead, we will be using effect encoding to replace the `player` column with the estimated effect of that predictor.

# Per-player statistics

::: columns
::: {.column width="50%"}
```{r effects, echo = FALSE, out.width = '100%', fig.width = 6, fig.height = 3, fig.align='center', dev = 'svg', dev.args = list(bg = "transparent")}
player_stats <- 
  nhl_train %>%
  group_by(player) %>%
  summarize(
    rate = mean(on_goal == "yes"), 
    num_shots = n(),
    .groups = "drop"
    ) %>%
  mutate(player = reorder(player, rate))
  
player_stats %>%   
  ggplot(aes(x = num_shots)) +
  geom_histogram(bins = 30, col = "blue", fill = "blue", alpha = 1/3) +
  scale_x_log10() +
  labs(x = "Number of shots per player")
player_stats %>%   
  ggplot(aes(x = rate)) +
  geom_histogram(binwidth = 1/40, col = "red", fill = "red", alpha = 1/3) +
  labs(x = "On-goal rate per player")
```
:::

::: {.column width="50%"}
There are good statistical methods for estimating these rates that use *partial pooling*.

This borrows strength across players and shrinks extreme values (e.g. zero or one) towards the mean for players with very few shots.

The embed package has recipes steps for effect encodings.
:::
:::

# Partial pooling

```{r effect-compare, echo = FALSE, out.width = '50%', fig.width = 4, fig.height = 4, fig.align='center', dev = 'svg', dev.args = list(bg = "transparent")}
library(embed)

estimates <- 
  recipe(on_goal ~ ., data = nhl_train) %>% 
  step_lencode_mixed(player, outcome = vars(on_goal)) %>%   #<<
  step_dummy(all_nominal_predictors()) %>% 
  step_zv(all_predictors()) %>% 
  step_ns(angle, deg_free = 10) %>% 
  step_ns(distance, deg_free = 10) %>% 
  step_normalize(all_numeric_predictors()) %>% 
  prep() %>% 
  tidy(number = 4) %>% 
  select(player = level, estimate = value)

inner_join(player_stats, estimates, by = "player") %>% 
  mutate(estimate = binomial()$linkinv(estimate)) %>% 
  ggplot(aes(x = rate, y = estimate)) + 
  geom_abline(col = "green", lty = 2) +
  geom_point(aes(size = num_shots), alpha = 1/3) +
  lims(x = 0:1, y = 0:1) +
  coord_fixed() +
  scale_size(range = c(1/3, 3)) +
  labs(x = "Raw Rate", y = "Estimated via Effects Encoding")
```

# The last recipe `r I(hexes(c("recipes", "embed")))`

```{r}
library(embed)

nhl_rec <- 
  recipe(on_goal ~ ., data = nhl_train) %>% 
  step_lencode_mixed(player, outcome = vars(on_goal)) %>%   #<<
  step_dummy(all_nominal_predictors()) %>% 
  step_zv(all_predictors()) %>% 
  step_ns(angle, deg_free = 10) %>% 
  step_ns(distance, deg_free = 10) %>% 
  step_normalize(all_numeric_predictors()) 
```

It is very important to appropriately validate the effect encoding step to make sure that we are not overfitting.

# Recipes are estimated

*Every* preprocessing step in a recipe that involved calculations uses the *training set*. For example:

-   Levels of a factor
-   Determination of zero-variance
-   Normalization
-   Feature extraction
-   Effect encodings

and so on.

Once a recipe is added to a workflow, this occurs when `fit()` is called.

# Debugging a recipe

90% of the time, you will want to use a workflow to estimate and apply a recipe.

If you have an error, the original recipe object (e.g. `nhl_rec`) can be estimated manually with a function called `prep()`. It is analogous to `fit()`.

This returns the fitted recipe. This can help debug any issues.

Another function (`bake()`) is analogous to `predict()` and gives you the processed data back.

# Fun facts about recipes

-   Once `fit()` is called on a workflow, changing the model does not re-fit the recipe.
-   A list of all known steps is [here](https://www.tidymodels.org/find/recipes/).
-   Some steps can be [skipped](https://recipes.tidymodels.org/articles/Skipping.html) when using `predict()`.
-   The [order](https://recipes.tidymodels.org/articles/Ordering.html) of the steps matters.
-   There are recipes-adjacent packages with more steps: embed, timetk, textrecipes, and others.
    -   If you do any text processing, textrecipes is `r emo::ji("cool")`<sup>`r emo::ji("infinity")`</sup>.
        -   Julia and Emil have written an amazing text processing book: [*Supervised Machine Learning for Text Analysis in R*](https://smltar.com/)
-   There are a lot of ways to handle [categorical predictors](https://recipes.tidymodels.org/articles/Dummies.html), even those with novel levels.
-   Several dplyr steps exist, such as `step_mutate()`.
