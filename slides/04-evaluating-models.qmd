---
title: "4 - Evaluating models"
subtitle: "Machine learning with tidymodels"
format:
  revealjs: 
    slide-number: true
    footer: <https://workshops.tidymodels.org>
    include-before-body: header.html
    theme: [default, tidymodels.scss]
knitr:
  opts_chunk: 
    echo: true
---

```{r}
#| include: false
#| file: setup.R
```

## Metrics for model performance `r hexes("yardstick")`

```{r}
#| echo: false
library(tidyverse)
library(tidymodels)
data("tree_frogs", package = "stacks")
tree_frogs <- tree_frogs %>%
  filter(!is.na(latency)) %>%
  select(-c(clutch, hatched))

set.seed(123)
frog_split <- initial_split(tree_frogs, prop = 0.8, strata = latency)
frog_train <- training(frog_split)
frog_test <- testing(frog_split)
tree_spec <- decision_tree(cost_complexity = 0.001, mode = "regression")
tree_wf <- workflow(latency ~ ., tree_spec)
tree_fit <- fit(tree_wf, frog_train)
```

```{r}
augment(tree_fit, new_data = frog_test) %>%
  metrics(latency, .pred)
```

. . .

-   RMSE: difference between the predicted and observed values ⬇️
-   $R^2$: squared correlation between the predicted and observed values ⬆️
-   MAE: similar to RMSE, but mean absolute error ⬇️

## Metrics for model performance `r hexes("yardstick")`

```{r}
augment(tree_fit, new_data = frog_test) %>%
  rmse(latency, .pred)
```

## Metrics for model performance `r hexes("yardstick")`

```{r}
augment(tree_fit, new_data = frog_test) %>%
  group_by(reflex) %>%
  rmse(latency, .pred)
```

## Metrics for model performance `r hexes("yardstick")`

```{r}
frog_metrics <- metric_set(rmse, msd)
augment(tree_fit, new_data = frog_test) %>%
  frog_metrics(latency, .pred)
```

##  {background-iframe="https://yardstick.tidymodels.org/reference/index.html"}

::: footer
:::

# We'll talk about classification metrics tomorrow!

## Which data set should we use? `r hexes("yardstick")`

::: columns
::: {.column width="50%"}
```{r}
tree_fit %>%
  augment(frog_train) %>%
  rmse(latency, .pred)
```
:::

::: {.column width="50%"}
:::
:::

## Which data set should we use? `r hexes("yardstick")`

::: columns
::: {.column width="50%"}
```{r}
tree_fit %>%
  augment(frog_train) %>%
  rmse(latency, .pred)
```
:::

::: {.column width="50%"}
```{r}
tree_fit %>%
  augment(frog_test) %>%
  rmse(latency, .pred)
```
:::
:::

## Which data set should we use? `r hexes("yardstick")`

::: columns
::: {.column width="50%"}
:::

::: {.column width="50%"}
```{r}
tree_fit %>%
  augment(frog_test) %>%
  rmse(latency, .pred)
```
:::
:::

## Which data set should we use? `r hexes("yardstick")`

```{r}
tree_fit %>%
  augment(frog_train)
```

We call this "resubstition" or "repredicting the training set"

## Which data set should we use? `r hexes("yardstick")`

```{r}
tree_fit %>%
  augment(frog_train) %>%
  rmse(latency, .pred)
```

We call this a "resubstition metric"

##  {background-image="https://media.giphy.com/media/55itGuoAJiZEEen9gg/giphy.gif" background-size="800px"}

# Decision tree 🌳

# Random forest 🌳🌲🌴🌵🌴🌳🌳🌴🌲🌵🌴🌲🌳🌴🌳🌵🌵🌴🌲🌲🌳🌴🌳🌴🌲🌴🌵🌴🌲🌴🌵🌲🌳

## Create a random forest model `r hexes("parsnip")`

```{r}
rf_spec <- rand_forest(trees = 1000, mode = "regression")
rf_spec
```

## Create a random forest model `r hexes("workflows")`

```{r}
rf_wf <- workflow(latency ~ ., rf_spec)
rf_wf
```

## Your turn {transition="slide-in"}

![](images/parsnip-flagger.jpg){.absolute top="0" right="0" width="150" height="150"}

*Fit `rf_wf` to the training data.*

*Use `augment()` and `metrics()` to compute metrics for both training and testing data.*

```{r}
#| echo: false
countdown(minutes = 5)
```

```{r}
#| echo: false
rf_fit <- fit(rf_wf, data = frog_train)
```

## Random forest metrics `r hexes("yardstick")`

::: columns
::: {.column width="50%"}
```{r}
rf_fit %>%
  augment(frog_train) %>%
  metrics(latency, .pred)
```
:::

::: {.column width="50%"}
```{r}
rf_fit %>%
  augment(frog_test) %>%
  metrics(latency, .pred)
```
:::
:::

## Comparing metrics `r hexes("yardstick")`

::: columns
::: {.column width="50%"}
```{r}
tree_fit %>%
  augment(frog_test) %>%
  metrics(latency, .pred)
```
:::

::: {.column width="50%"}
```{r}
rf_fit %>%
  augment(frog_test) %>%
  metrics(latency, .pred)
```
:::
:::

. . .

What if we want to compare more models? 

. . .

And/or more model configurations?

. . .

And we want to understand if these are important differences?

# The testing data is precious 💎

# How can we use the *training* data to compare and evaluate different models? 🤔

##  {background-color="white" background-image="https://www.tmwr.org/premade/resampling.svg" background-size="1000px"}
